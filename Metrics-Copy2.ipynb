{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the evaluation information with your saved runs\n",
    "\n",
    "You can test your own codes by adding a folder in saved folder containing real.tsv and pred.tsv\n",
    "the tsv format is:\n",
    "```\n",
    "filename    onset   offset     event_label\n",
    "```\n",
    "Then by running the following cell you will see your folder\n",
    "\n",
    "For demo some saved data are presented in the saved folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the following code will display the results on AR (on the similar dataset to the paper) \n",
    "The result maybe a bit different from the result on the paper because in the paper it has been run several time and the result here is only one run. If you want to see full result please run the pipeline folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T18:59:17.248472Z",
     "start_time": "2021-11-16T18:59:16.211173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a89a33b61a497880fd98d8f57d7d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='file', options=('0-HHMM--Home1', '0-HHMM--Home2', '0-wardpaper', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual,widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ar_mme_eval.multi_eval\n",
    "import ar_mme_eval.ui as ui\n",
    "\n",
    "rootFolder='saved'\n",
    "debug=0 # to show debug information\n",
    "args={\n",
    "    'theta_tp':0,\n",
    "    'theta_fp':1,\n",
    "    'beta_s':2,\n",
    "    'beta_e':2,\n",
    "    'show_sed':False,\n",
    "    'plot_ward':False,\n",
    "    'show tp/fp/fn':False\n",
    "}\n",
    "\n",
    "\n",
    "@interact\n",
    "def result_selector(file=sorted([f for f in os.listdir(rootFolder) if os.path.isfile(f\"{rootFolder}/{f}/real.tsv\")])):\n",
    "    if(file==None):return\n",
    "    \n",
    "    folder=f'{rootFolder}/{file}'\n",
    "    gtf=f'{folder}/real.tsv'\n",
    "    global groundtruth\n",
    "    groundtruth=pd.read_csv(gtf,delimiter='\\t')\n",
    "    allClass=list(groundtruth['event_label'].unique()); allClass.sort();  allClass.append('macro-avg')\n",
    "    if 'None' in allClass: allClass.remove('None');allClass.append('None')\n",
    "    info_file='/tmp/metric_info.pkl';  info={}\n",
    "    import compress_pickle\n",
    "    if (os.path.exists(info_file)): info = compress_pickle.load(info_file)\n",
    "    print('Please input a Display name for each run')\n",
    "    runs_widget={}\n",
    "    for runfile in sorted(os.listdir(f'{folder}')):\n",
    "        if runfile=='real.tsv':continue\n",
    "        run=runfile.replace('.tsv','')\n",
    "        runs_widget[run]=widgets.Text(info.get(file+run,run),description=run)\n",
    "        display(widgets.HBox([runs_widget[run],widgets.Label(run)]))\n",
    "    ui.display_dataset_info(groundtruth)\n",
    "    @interact_manual\n",
    "    def result_selector(claz=widgets.SelectMultiple(options=allClass, description='classes')):\n",
    "        compress_pickle.dump({**info,**{file+r:runs_widget[r].value  for r in runs_widget}}, info_file)\n",
    "        global total_dic\n",
    "        if 10:\n",
    "            total_dic={}\n",
    "            for run in runs_widget:\n",
    "                print(f'')\n",
    "                print(f'    {run}')\n",
    "                pef = f'{folder}/{run}.tsv'\n",
    "                print(pef)\n",
    "                displayName=runs_widget[run].value\n",
    "                if(displayName==''):continue\n",
    "                try:\n",
    "                    res1=ar_mme_eval.multi_eval.get_single_result(gtf,pef,None,debug=debug,args=args)\n",
    "                    if(displayName in total_dic): print(f\"Warning! a name you have selected is repeated {run}:{displayName}\")\n",
    "                    total_dic[displayName]=res1\n",
    "                except Exception as e:\n",
    "                    print('Error! submission is ignored',e)\n",
    "                    raise \n",
    "\n",
    "        runs=list(total_dic.keys());runs.sort()\n",
    "        props=list(total_dic[runs[0]].keys());#props.sort()\n",
    "        metric_groups={g:[p.split(\": \")[1] for p in props if g in p] for g in np.unique([p.split(\": \")[0] for p in props])}\n",
    "        \n",
    "#         claz=['Take_Medicine']\n",
    "        with pd.ExcelWriter('output.xlsx') as writer:  \n",
    "            for g in metric_groups:\n",
    "                fs={c:pd.concat({run:pd.concat({prop:total_dic[run][g+\": \"+prop].loc[c] for prop in metric_groups[g]},axis=1).T for run in runs},axis=1) for c in claz} \n",
    "                global f1\n",
    "                f1=(pd.concat(fs,axis=1))\n",
    "                f1.to_excel(writer, sheet_name=g)\n",
    "                print(f'=============={g} Metrics ==================')\n",
    "                f1.index.name=g\n",
    "                \n",
    "                if(g=='Our'):\n",
    "                    props_short= {'detection-pakdd':'D', 'uniformity-pakdd':'U', 'total duration':'T',  'relative duration-pakdd':'R', 'boundary start-pakdd':'BS','boundary end-pakdd':'BE'}\n",
    "                    display(f1.loc[[p for p in props_short]])\n",
    "                    fs2={c:{run:{props_short[prop]:f1[c,run].loc[prop]['f1'] for prop in props_short} for run in runs} for c in claz}\n",
    "                    ui.plot_multi_spider(fs2)\n",
    "                    \n",
    "                else:\n",
    "                    display(f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
